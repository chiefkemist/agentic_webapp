#+title: Creating A Webapp and Refining Agent Processes with Langchain and Langgraph
#+author: ChiefKemist
#+date: <2024-08-05 Mon>


* Introduction

New / Newer tools appeared:

+ Someone please write html for me
+ Someone please write js for me
+ Someone please write styles for me
+ Someone please orchestrate my computations for me
+ DSLs reduce the cognitive burdern but Macros can do more (Odd similarities)
+ Essential protocols for streaming UIs (HTTP is always relevant)
+ Combining Generators + AsyncIO + Streaming HTTP protocols

* Improving the Ergonomics


** Model selection

#+begin_src python

from enum import Enum
from functools import cache

from langchain_anthropic import ChatAnthropic
from langchain_groq import ChatGroq
from langchain_openai import ChatOpenAI


class LLMModel(str, Enum):
    Claude3_Opus = "claude-3-opus-20240229"
    Claude35_Sonnet = "claude-3-5-sonnet-20240620"
    Claude3_Haiku = "claude-3-haiku-20240307"
    GPT4_Omni = "gpt-4o"
    GPT4_Omni_mini = "gpt-4o-mini"
    GPT35_Turbo = "gpt-3.5-turbo"
    LLAMA31_70b = "llama-3.1-70b-versatile"
    LLAMA31_8b = "llama-3.1-8b-instant"
    LLAMA3_70b = "llama3-70b-8192"
    LLAMA3_8b = "llama3-8b-8192"


@cache
def get_llm(model_name: LLMModel):
    llm = {
        LLMModel.Claude3_Opus: ChatAnthropic(model_name=LLMModel.Claude3_Opus),
        LLMModel.Claude35_Sonnet: ChatAnthropic(model_name=LLMModel.Claude35_Sonnet),
        LLMModel.Claude3_Haiku: ChatAnthropic(model_name=LLMModel.Claude3_Haiku),
        LLMModel.GPT4_Omni: ChatOpenAI(model_name=LLMModel.GPT4_Omni),
        LLMModel.GPT4_Omni_mini: ChatOpenAI(model_name=LLMModel.GPT4_Omni_mini),
        LLMModel.GPT35_Turbo: ChatOpenAI(model_name=LLMModel.GPT35_Turbo),
        LLMModel.LLAMA31_70b: ChatGroq(model_name=LLMModel.LLAMA31_70b),
        LLMModel.LLAMA31_8b: ChatGroq(model_name=LLMModel.LLAMA31_8b),
        LLMModel.LLAMA3_70b: ChatGroq(model_name=LLMModel.LLAMA3_70b),
        LLMModel.LLAMA3_8b: ChatGroq(model_name=LLMModel.LLAMA3_8b),
    }.get(model_name, None)

    if llm is None:
        raise ValueError(f"Model {model_name} not found")

    return llm

#+end_src

** Prompt Engineering

Discuss Prompt Poet vs Langchain Hub here...

** Structured Output

#+begin_src python

class SampleCode(BaseModel):
    markdown: str = Field(
        ..., title="Markdown", description="The markdown explanation of the code"
    )
    code: str = Field(
        ...,
        title="Code",
        description="The code snippet which implements the functionality",
    )

llm = get_llm(LLMModel.GPT4_Omni_mini)
structured_llm = llm.with_structured_output(SampleCode)

#+end_src

** Agent class


#+begin_src python


class AgentState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]


class Agent:
    def __init__(
        self,
        name,
        model: LLMModel,
        system="",
        tools=[],
        output_structure=None,
        delegates: Dict[str, Any] = None,
    ):
        print_debug_msg(f"Initializing agent with model {model}")
        self.system = system
        llm = get_llm(model)
        graph_builder = graph.StateGraph(AgentState)
        graph_builder.add_node(name, self.call_llm)
        graph_builder.add_node("action", self.act)
        if output_structure:
            graph_builder.add_node("output_parser", self.output_parser)
            graph_builder.add_conditional_edges(
                name, self.should_act, {True: "action", False: "output_parser"}
            )
            graph_builder.set_finish_point("output_parser")
        else:
            graph_builder.add_conditional_edges(
                name, self.should_act, {True: "action", False: END}
            )
        graph_builder.add_edge("action", name)
        graph_builder.set_entry_point(name)
        self.graph = graph_builder.compile()
        if len(tools) > 0:
            self.tools = {tool.name: tool for tool in tools}
            self.llm = llm.bind_tools(tools)
        else:
            self.llm = llm
        self.output_structure = output_structure

    def output_parser(self, state: AgentState):
        print_debug_msg(f"Output parser with state {state['messages']}")
        llm_with_output_structure = self.llm.with_structured_output(
            self.output_structure
        )
        structured_output = llm_with_output_structure.invoke(state["messages"])
        return {"messages": structured_output.json()}

    def should_act(self, state: AgentState):
        print_debug_msg(f"Checking if action exists in {state['messages']}")
        result = state["messages"][-1]
        tool_calls_count = len(result.tool_calls)
        return tool_calls_count > 0

    def call_llm(self, state: AgentState):
        print_debug_msg(f"Calling LLM with state {state}")
        messages = state["messages"]
        if self.system:
            messages = [SystemMessage(content=self.system)] + messages
        message = self.llm.invoke(messages)
        return {"messages": [message]}

    def act(self, state: AgentState):
        print_debug_msg(f"Taking action on message {state['messages'][-1]}")
        tool_calls = state["messages"][-1].tool_calls
        results = []
        for t in tool_calls:
            print_debug_msg(f"Calling: {t}")
            if not t["name"] in self.tools:
                print_error_msg(f"Tool {t['name']} not found")
                result = "Tool not found, please try again"
            else:
                result = self.tools[t["name"]].invoke(t["args"])
            results.append(
                ToolMessage(tool_call_id=t["id"], name=t["name"], content=str(result))
            )
        print_debug_msg("Back to model after action")
        return {"messages": results}

    def __call__(
        self, message: HumanMessage, stream=False, debug=False
    ) -> Iterator[dict]:
        if stream:
            results = self.graph.stream(dict(messages=message), debug=debug)
            return results
        else:
            results = self.graph.invoke(dict(messages=message), debug=debug)
            return results

#+end_src

* Streaming UIs

** Python + Starlette + HTMX => FastHTML

Not that different than what we are used to coming from FastAPI + Jinja2 + HTMX. Main difference is the nice Python DSL for HTML.

*** Build HTML pages in Python

TODO

*** Use HTMX from within FastHTML

TODO

*** Render FastHTML to string


#+begin_src python :results output

from fasthtml.common import to_xml
from fasthtml import P, Div, Article

comp = to_xml(Article(Div("Lambert!", P("Rigobert"))))

print(comp)

#+end_src

#+RESULTS:
: <article>
:   <div>
: Lambert!
:     <p>Rigobert</p>
:   </div>
: </article>
:

*** Streaming UIs requirements

**** AsyncIO

TODO

**** Generators

TODO

**** Server Sent Events (SSE)

TODO

**** SSE Message

#+begin_src python :results output

from fasthtml.common import to_xml
from fasthtml import Div

def render_sse_html_chunk(event: str, id: str, chunk: str) -> bytes:
    return f"""
event: {event}
data: {to_xml(Div(chunk, id=id, hx_swap_oob='true'))}\n\n
""".encode("utf-8")

sse_msg = render_sse_html_chunk("SomeEvent", "SomeID", "Chunk (string or html string or whatever data as string)")

print(sse_msg)

#+end_src

#+RESULTS:
: b'\nevent: SomeEvent\ndata: <div hx-swap-oob="true" id="SomeID">Chunk (string or html string or whatever data as string)</div>\n\n\n\n'

**** Terminate Stream

TODO


* Prior Work on Streaming UIs

+ [[https://www.youtube.com/watch?v=nSMgm0YSLOA][UbuntuTechHive -- ðŸ¤– Implementing a Custom Chatbot with OpenAI API and Python (2024-01-13)]]
+ [[https://www.youtube.com/watch?v=1I_oDsEDwa8][UbuntuTechHive -- ðŸ¤– Implementing a Custom Chatbot with OpenAI API and Python Part-Deux (2024-01-27)]]
+ [[https://www.youtube.com/watch?v=NDuTWN5a_78][UbntTH -- Real-time Data Visualizations with Python, HTMX and LLM generated SQL queries (2024-06-01)]]

* References

+ [[https://15799.courses.cs.cmu.edu/fall2013/static/papers/p135-malewicz.pdf][Pregel]]: A System for Large-Scale Graph Processing
+ [[https://smith.langchain.com/hub][LangChain Hub]]: Explore and contribute prompts to the community hub.
+ [[https://github.com/character-ai/prompt-poet?ref=research.character.ai][Prompt Poet]]: Streamlines and simplifies prompt design for both developers and non-technical users with a low code approach.
+ [[https://docs.fastht.ml/][FastHTML]]: The fastest, most powerful way to create an HTML app.
